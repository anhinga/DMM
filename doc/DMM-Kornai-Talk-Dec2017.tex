\documentclass{beamer}

\usepackage{beamerthemeshadow}
\usepackage{color}
\usepackage{verbatim}
\usepackage{tikz}
%\usepackage{wrapfig}

\mode<presentation>
{
 \usetheme{Warsaw} %%% Change later
\usecolortheme{dove}

%gets rid of bottom navigation bars
\setbeamertemplate{footline}[page number]{}

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}


\usepackage{graphicx}

\usepackage{amssymb,soul}

\usepackage{hyperref}

\definecolor{myblue}{rgb}{0, 0, 0.9}

\definecolor{mygreen}{rgb}{0,0.9,0}

\definecolor{mymagenta}{rgb}{0.9, 0, 0.9}

\definecolor{myred}{rgb}{0.9,0,0}

\definecolor{mygray}{rgb}{0.8,0.8,0.8}

\definecolor{mydark}{rgb}{0.3, 0.3, 0.3}

\newcommand{\msblue}[1]{{\color{myblue} #1}}

\newcommand{\msmagenta}[1]{{\color{mymagenta} #1}}

\newcommand{\msred}[1]{{\color{myred} #1}}

\newcommand{\msgreen}[1]{{\color{mygreen} #1}}

\newcommand{\msgray}[1]{{\color{mygray} #1}}

\newcommand{\msdark}[1]{{\color{mydark} #1}}

\newcommand{\relu}{\mbox{\bf ReLU}}

\begin{document}

\title{Dataflow Matrix Machines and V-values:\\ a Bridge between Programs and Neural Nets}
\author{\bf Michael Bukatin}
\institute[HERE] % (optional, but mostly needed)
{
 % \inst{1}%
{\small HERE Technologies}
}
\date[]  
{\small Joint work with Jon Anthony (Boston College)\\
 - - -\\
{Video presentation for K\'alm\'an \& Kornai workshop,\\
Budapest, December 18, 2017}}

\begin{frame}
  \titlepage
\end{frame}



\section{Introduction}



\begin{frame}

\frametitle{Dataflow matrix machines (DMMs)}

\begin{itemize}

\item A strong generalization of neural networks\\[2ex]

\msgray{\item Powerful enough  to write programs in this formalism\\[2ex]

\item A program is determined by a matrix of numbers\\[2ex]

\item Any program $P$ can be transformed to any program $Q$\\ in this formalism
        by continuously transforming\\ the matrices defining those programs.\\[2ex]

\item Synthesize a matrix of numbers to synthesize a program.

}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Dataflow matrix machines (DMMs)}

\begin{itemize}

\item A strong generalization of neural networks\\[2ex]

\item Powerful enough  to write programs in this formalism\\[2ex]

\msgray{\item A program is determined by a matrix of numbers\\[2ex]

\item Any program $P$ can be transformed to any program $Q$\\ in this formalism
        by continuously transforming\\ the matrices defining those programs.\\[2ex]

\item Synthesize a matrix of numbers to synthesize a program.

}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Dataflow matrix machines (DMMs)}

\begin{itemize}

\item A strong generalization of neural networks\\[2ex]

\item Powerful enough  to write programs in this formalism\\[2ex]

\item A program is determined by a matrix of numbers\\[2ex]

\msgray{\item Any program $P$ can be transformed to any program $Q$\\ in this formalism
        by continuously transforming\\ the matrices defining those programs.\\[2ex]

\item Synthesize a matrix of numbers to synthesize a program.

}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Dataflow matrix machines (DMMs)}

\begin{itemize}

\item A strong generalization of neural networks\\[2ex]

\item Powerful enough  to write programs in this formalism\\[2ex]

\item A program is determined by a matrix of numbers\\[2ex]

\item Any program $P$ can be transformed to any program $Q$\\ in this formalism
        by continuously transforming\\ the matrices defining those programs.\\[2ex]

\msgray{\item Synthesize a matrix of numbers to synthesize a program.

}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Dataflow matrix machines (DMMs)}

\begin{itemize}

\item A strong generalization of neural networks\\[2ex]

\item Powerful enough  to write programs in this formalism\\[2ex]

\item A program is determined by a matrix of numbers\\[2ex]

\item Any program $P$ can be transformed to any program $Q$\\ in this formalism
        by continuously transforming\\ the matrices defining those programs.\\[2ex]

\item Synthesize a matrix of numbers to synthesize a program.

\end{itemize}

\end{frame}



\begin{frame}

\frametitle{Linear streams}

\begin{itemize}

\item Neural networks process streams of numbers\\[2ex]

\msgray{\item DMMs process linear streams\\[2ex]

\item Streams are sequences\\[2ex]

\item Linear streams: the notion of\\ linear combination of several streams is defined\\[2ex]

\item Example: for a vector space $V$,\\ streams of its elements form a space of linear streams\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Linear streams}

\begin{itemize}

\item Neural networks process streams of numbers\\[2ex]

\item DMMs process linear streams\\[2ex]

\msgray{\item Streams are sequences\\[2ex]

\item Linear streams: the notion of\\ linear combination of several streams is defined\\[2ex]

\item Example: for a vector space $V$,\\ streams of its elements form a space of linear streams\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Linear streams}

\begin{itemize}

\item Neural networks process streams of numbers\\[2ex]

\item DMMs process linear streams\\[2ex]

\item Streams are sequences\\[2ex]

\msgray{\item Linear streams: the notion of\\ linear combination of several streams is defined\\[2ex]

\item Example: for a vector space $V$,\\ streams of its elements form a space of linear streams\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Linear streams}

\begin{itemize}

\item Neural networks process streams of numbers\\[2ex]

\item DMMs process linear streams\\[2ex]

\item Streams are sequences\\[2ex]

\item Linear streams: the notion of\\ linear combination of several streams is defined\\[2ex]

\msgray{\item Example: for a vector space $V$,\\ streams of its elements form a space of linear streams\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Linear streams}

\begin{itemize}

\item Neural networks process streams of numbers\\[2ex]

\item DMMs process linear streams\\[2ex]

\item Streams are sequences\\[2ex]

\item Linear streams: the notion of\\ linear combination of several streams is defined\\[2ex]

\item Example: for a vector space $V$,\\ streams of its elements form a space of linear streams\\[2ex]

\end{itemize}

\end{frame}



\begin{frame}

\frametitle{Kinds of linear streams}

\begin{itemize}

\item To distinguish between different spaces of linear streams we talk about
kinds of linear streams.\\[2ex]

\msgray{\item Every vector space $V$ gives rise to the corresponding kind of linear streams
(streams of vectors from that space)\\[2ex]

\item Every measurable space $X$ gives rise to the space of streams of
probabilistic samples drawn from $X$ and decorated with +/- signs
(linear combination is defined by a stochastic procedure)\\[2ex]

\item Streams of images of a particular size (that is, animations)\\[2ex]

\item Streams of matrices; streams of multidimensional arrays\\[2ex]

\item Most importantly: streams of V-values\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Kinds of linear streams}

\begin{itemize}

\item To distinguish between different spaces of linear streams we talk about
kinds of linear streams.\\[2ex]

\item Every vector space $V$ gives rise to the corresponding kind of linear streams
(streams of vectors from that space)\\[2ex]

\msgray{\item Every measurable space $X$ gives rise to the space of streams of
probabilistic samples drawn from $X$ and decorated with +/- signs
(linear combination is defined by a stochastic procedure)\\[2ex]

\item Streams of images of a particular size (that is, animations)\\[2ex]

\item Streams of matrices; streams of multidimensional arrays\\[2ex]

\item Most importantly: streams of V-values\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Kinds of linear streams}

\begin{itemize}

\item To distinguish between different spaces of linear streams we talk about
kinds of linear streams.\\[2ex]

\item Every vector space $V$ gives rise to the corresponding kind of linear streams
(streams of vectors from that space)\\[2ex]

\item Every measurable space $X$ gives rise to the space of streams of
probabilistic samples drawn from $X$ and decorated with +/- signs
(linear combination is defined by a stochastic procedure)\\[2ex]

\msgray{\item Streams of images of a particular size (that is, animations)\\[2ex]

\item Streams of matrices; streams of multidimensional arrays\\[2ex]

\item Most importantly: streams of V-values\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Kinds of linear streams}

\begin{itemize}

\item To distinguish between different spaces of linear streams we talk about
kinds of linear streams.\\[2ex]

\item Every vector space $V$ gives rise to the corresponding kind of linear streams
(streams of vectors from that space)\\[2ex]

\item Every measurable space $X$ gives rise to the space of streams of
probabilistic samples drawn from $X$ and decorated with +/- signs
(linear combination is defined by a stochastic procedure)\\[2ex]

\item Streams of images of a particular size (that is, animations)\\[2ex]

\msgray{\item Streams of matrices; streams of multidimensional arrays\\[2ex]

\item Most importantly: streams of V-values\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Kinds of linear streams}

\begin{itemize}

\item To distinguish between different spaces of linear streams we talk about
kinds of linear streams.\\[2ex]

\item Every vector space $V$ gives rise to the corresponding kind of linear streams
(streams of vectors from that space)\\[2ex]

\item Every measurable space $X$ gives rise to the space of streams of
probabilistic samples drawn from $X$ and decorated with +/- signs
(linear combination is defined by a stochastic procedure)\\[2ex]

\item Streams of images of a particular size (that is, animations)\\[2ex]

\item Streams of matrices; streams of multidimensional arrays\\[2ex]

\msgray{\item Most importantly: streams of V-values\\[2ex]


}

\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Kinds of linear streams}

\begin{itemize}

\item To distinguish between different spaces of linear streams we talk about
kinds of linear streams.\\[2ex]

\item Every vector space $V$ gives rise to the corresponding kind of linear streams
(streams of vectors from that space)\\[2ex]

\item Every measurable space $X$ gives rise to the space of streams of
probabilistic samples drawn from $X$ and decorated with +/- signs
(linear combination is defined by a stochastic procedure)\\[2ex]

\item Streams of images of a particular size (that is, animations)\\[2ex]

\item Streams of matrices; streams of multidimensional arrays\\[2ex]

\item Most importantly: streams of V-values\\[2ex]



\end{itemize}

\end{frame}

\section{Neural model of computation}

\begin{frame}

  \frametitle{Recurrent neural networks}



\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{tikzpicture}
   \clip (-2.0, -2.0) rectangle (4.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
   \draw [->] (-0.8, 0) -- (-0.8, 1) node [left] {$i_m$};
   \draw [->] (-1.6, 0) -- (-1.6, 1) node [left] {$i_1$};
   \filldraw (-1.2,0) circle [radius=0.5pt]
                 (-1.4,0)  circle [radius=0.5pt]
                 (-1.0, 0) circle [radius=0.5pt]
                 (-0.8, 0) circle [radius=1pt]
                 (-1.6, 0) circle [radius=1pt];

 

   \draw [->] (-0.4, -1) node [left] {$x_1$} -- (-0.4, 1) node [midway, above right] {$f_1$} node [right] {$y_1$};
   \draw [->] (0.4, -1)  node [left] {$x_k$} -- (0.4, 1) node [midway, above right] {$f_k$} node [right] {$y_k$};
   \filldraw (0,0) circle [radius=0.5pt]
                 (-0.2,0)  circle [radius=0.5pt]
                 (0.2, 0) circle [radius=0.5pt];

   \draw (0.8, -1) node [right] {$o_1$} -- (0.8, 0);
   \draw (1.6, -1) node [right] {$o_n$} -- (1.6, 0);
   \filldraw (1.2,0) circle [radius=0.5pt]
                 (1.4,0)  circle [radius=0.5pt]
                 (1.0, 0) circle [radius=0.5pt]
                 (0.8, 0) circle [radius=1pt]
                 (1.6, 0) circle [radius=1pt]; 

  \draw [->, very thick] (0, 1.2) .. controls (3.5, 4.2) and (3.5, -4.2) .. (0, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.5\textwidth}

\msgray{``Two-stroke engine" for an RNN:\\[3ex]


``Down movement": $(x_1^{t+1}, \ldots, x_k^{t+1}, o_1^{t+1}, \ldots, o_n^{t+1})^{\top}=$\\[0.1ex] {\bf W}$ \cdot (y_1^{t}, \ldots, y_k^{t}, i_1^{t}, \ldots, i_m^{t})^{\top}$.\\[3ex]

``Up movement":\\[0.1ex]  $y_1^{t+1} = f_1(x_1^{t+1}), \ldots,$\\[0.1ex]$y_k^{t+1} = f_k(x_k^{t+1})$.}

\end{column}
\end{columns}


\end{frame}

\begin{frame}

  \frametitle{Recurrent neural networks}



\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{tikzpicture}
   \clip (-2.0, -2.0) rectangle (4.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
   \draw [->] (-0.8, 0) -- (-0.8, 1) node [left] {$i_m$};
   \draw [->] (-1.6, 0) -- (-1.6, 1) node [left] {$i_1$};
   \filldraw (-1.2,0) circle [radius=0.5pt]
                 (-1.4,0)  circle [radius=0.5pt]
                 (-1.0, 0) circle [radius=0.5pt]
                 (-0.8, 0) circle [radius=1pt]
                 (-1.6, 0) circle [radius=1pt];

 

   \draw [->] (-0.4, -1) node [left] {$x_1$} -- (-0.4, 1) node [midway, above right] {$f_1$} node [right] {$y_1$};
   \draw [->] (0.4, -1)  node [left] {$x_k$} -- (0.4, 1) node [midway, above right] {$f_k$} node [right] {$y_k$};
   \filldraw (0,0) circle [radius=0.5pt]
                 (-0.2,0)  circle [radius=0.5pt]
                 (0.2, 0) circle [radius=0.5pt];

   \draw (0.8, -1) node [right] {$o_1$} -- (0.8, 0);
   \draw (1.6, -1) node [right] {$o_n$} -- (1.6, 0);
   \filldraw (1.2,0) circle [radius=0.5pt]
                 (1.4,0)  circle [radius=0.5pt]
                 (1.0, 0) circle [radius=0.5pt]
                 (0.8, 0) circle [radius=1pt]
                 (1.6, 0) circle [radius=1pt]; 

  \draw [->, very thick] (0, 1.2) .. controls (3.5, 4.2) and (3.5, -4.2) .. (0, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.5\textwidth}

``Two-stroke engine" for an RNN:\\[3ex]


``Down movement": $(x_1^{t+1}, \ldots, x_k^{t+1}, o_1^{t+1}, \ldots, o_n^{t+1})^{\top}=$\\[0.1ex] {\bf W}$ \cdot (y_1^{t}, \ldots, y_k^{t}, i_1^{t}, \ldots, i_m^{t})^{\top}$.\\[3ex]

\msgray{``Up movement":\\[0.1ex]  $y_1^{t+1} = f_1(x_1^{t+1}), \ldots,$\\[0.1ex]$y_k^{t+1} = f_k(x_k^{t+1})$.}

\end{column}
\end{columns}


\end{frame}

\begin{frame}

  \frametitle{Recurrent neural networks}



\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{tikzpicture}
   \clip (-2.0, -2.0) rectangle (4.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
   \draw [->] (-0.8, 0) -- (-0.8, 1) node [left] {$i_m$};
   \draw [->] (-1.6, 0) -- (-1.6, 1) node [left] {$i_1$};
   \filldraw (-1.2,0) circle [radius=0.5pt]
                 (-1.4,0)  circle [radius=0.5pt]
                 (-1.0, 0) circle [radius=0.5pt]
                 (-0.8, 0) circle [radius=1pt]
                 (-1.6, 0) circle [radius=1pt];

 

   \draw [->] (-0.4, -1) node [left] {$x_1$} -- (-0.4, 1) node [midway, above right] {$f_1$} node [right] {$y_1$};
   \draw [->] (0.4, -1)  node [left] {$x_k$} -- (0.4, 1) node [midway, above right] {$f_k$} node [right] {$y_k$};
   \filldraw (0,0) circle [radius=0.5pt]
                 (-0.2,0)  circle [radius=0.5pt]
                 (0.2, 0) circle [radius=0.5pt];

   \draw (0.8, -1) node [right] {$o_1$} -- (0.8, 0);
   \draw (1.6, -1) node [right] {$o_n$} -- (1.6, 0);
   \filldraw (1.2,0) circle [radius=0.5pt]
                 (1.4,0)  circle [radius=0.5pt]
                 (1.0, 0) circle [radius=0.5pt]
                 (0.8, 0) circle [radius=1pt]
                 (1.6, 0) circle [radius=1pt]; 

  \draw [->, very thick] (0, 1.2) .. controls (3.5, 4.2) and (3.5, -4.2) .. (0, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.5\textwidth}

``Two-stroke engine" for an RNN:\\[3ex]


``Down movement": $(x_1^{t+1}, \ldots, x_k^{t+1}, o_1^{t+1}, \ldots, o_n^{t+1})^{\top}=$\\[0.1ex] {\bf W}$ \cdot (y_1^{t}, \ldots, y_k^{t}, i_1^{t}, \ldots, i_m^{t})^{\top}$.\\[3ex]

``Up movement":\\[0.1ex]  $y_1^{t+1} = f_1(x_1^{t+1}), \ldots,$\\[0.1ex]$y_k^{t+1} = f_k(x_k^{t+1})$.

\end{column}
\end{columns}


\end{frame}




\begin{frame}

  \frametitle{Dataflow matrix machines}

Countable network with finite active part at any moment of time.\\[2ex]

\msgray{Countable matrix {\bf W} with finite number of non-zero elements at any moment of time.

\begin{columns}[T]
\begin{column}{0.7\textwidth}
\begin{tikzpicture}
   \clip (-3.0, -2.0) rectangle (5.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
  \filldraw (-3.2,0) circle [radius=0.5pt]
                (-3.0,0)  circle [radius=0.5pt]
                (-3.4, 0) circle [radius=0.5pt]; 

   \draw [->] (-1.2, 0) -- (-0.8, 1) node [right] {$y_{2,C_1}$};
   \draw [->] (-1.2, 0) -- (-1.6, 1) node [left] {$y_{1,C_1}$};
   \draw (-2.0, -1)  node [left] {$x_{1,C_1}$} -- (-1.2, 0);
   \draw (-1.2, -1)  node [below] {$x_{2,C_1}$} -- (-1.2, 0) node [left] {$f_{C_1}$};
   \draw (-0.4, -1)  node [right] {$x_{3,C_1}$} -- (-1.2, 0);


  \filldraw (0,0) circle [radius=0.5pt]
                (-0.2,0)  circle [radius=0.5pt]
                (0.2, 0) circle [radius=0.5pt];


   \draw (1.6, -1) node [left] {$x_{1,C_2}$} -- (2.0, 0)  node [left] {$f_{C_2}$};;
   \draw (2.4, -1) node [right] {$x_{2,C_2}$} -- (2.0, 0);
   \draw [->] (2.0, 0) -- (1.2, 1) node [left] {$y_{1,C_2}$};
   \draw [->] (2.0, 0) -- (2.0, 1) node [above] {$y_{2,C_2}$};
   \draw [->] (2.0, 0) -- (2.8, 1) node [right] {$y_{3,C_2}$};


  \filldraw (3.2,0) circle [radius=0.5pt]
                (3.0,0)  circle [radius=0.5pt]
                (3.4, 0) circle [radius=0.5pt]; 


  \draw [->, very thick] (0.5, 1.2) .. controls (5.5, 4.2) and (5.5, -4.2) .. (0.5, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.3\textwidth}

{%\scriptsize
\tiny
``Down movement": 

For all inputs $x_{i,C_k}$ where there is a non-zero weight $w_{(i,C_k), (j,C_l)}^t$:\\[1ex]

{\scriptsize$x_{i,C_k}^{t+1} = \sum_{\{(j,C_l) | w_{(i,C_k), (j,C_l)}^t \neq 0\}}$\\[0.1ex]$w_{(i,C_k), (j,C_l)}^t * y_{j, C_l}^{t}.$}\\[3ex]

``Up movement":\\[0.1ex]  

For all active neurons $C$:\\[1ex]

{\scriptsize $y^{t+1}_{1,C},...,y^{t+1}_{n,C} = f_C (x^{t+1}_{1,C},...,x^{t+1}_{m,C})$.}
}

\end{column}
\end{columns}
}


\end{frame}

\begin{frame}

  \frametitle{Dataflow matrix machines}

Countable network with finite active part at any moment of time.\\[2ex]

Countable matrix {\bf W} with finite number of non-zero elements at any moment of time.

\msgray{
\begin{columns}[T]
\begin{column}{0.7\textwidth}
\begin{tikzpicture}
   \clip (-3.0, -2.0) rectangle (5.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
  \filldraw (-3.2,0) circle [radius=0.5pt]
                (-3.0,0)  circle [radius=0.5pt]
                (-3.4, 0) circle [radius=0.5pt]; 

   \draw [->] (-1.2, 0) -- (-0.8, 1) node [right] {$y_{2,C_1}$};
   \draw [->] (-1.2, 0) -- (-1.6, 1) node [left] {$y_{1,C_1}$};
   \draw (-2.0, -1)  node [left] {$x_{1,C_1}$} -- (-1.2, 0);
   \draw (-1.2, -1)  node [below] {$x_{2,C_1}$} -- (-1.2, 0) node [left] {$f_{C_1}$};
   \draw (-0.4, -1)  node [right] {$x_{3,C_1}$} -- (-1.2, 0);


  \filldraw (0,0) circle [radius=0.5pt]
                (-0.2,0)  circle [radius=0.5pt]
                (0.2, 0) circle [radius=0.5pt];


   \draw (1.6, -1) node [left] {$x_{1,C_2}$} -- (2.0, 0)  node [left] {$f_{C_2}$};;
   \draw (2.4, -1) node [right] {$x_{2,C_2}$} -- (2.0, 0);
   \draw [->] (2.0, 0) -- (1.2, 1) node [left] {$y_{1,C_2}$};
   \draw [->] (2.0, 0) -- (2.0, 1) node [above] {$y_{2,C_2}$};
   \draw [->] (2.0, 0) -- (2.8, 1) node [right] {$y_{3,C_2}$};


  \filldraw (3.2,0) circle [radius=0.5pt]
                (3.0,0)  circle [radius=0.5pt]
                (3.4, 0) circle [radius=0.5pt]; 


  \draw [->, very thick] (0.5, 1.2) .. controls (5.5, 4.2) and (5.5, -4.2) .. (0.5, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.3\textwidth}

{%\scriptsize
\tiny
``Down movement": 

For all inputs $x_{i,C_k}$ where there is a non-zero weight $w_{(i,C_k), (j,C_l)}^t$:\\[1ex]

{\scriptsize$x_{i,C_k}^{t+1} = \sum_{\{(j,C_l) | w_{(i,C_k), (j,C_l)}^t \neq 0\}}$\\[0.1ex]$w_{(i,C_k), (j,C_l)}^t * y_{j, C_l}^{t}.$}\\[3ex]

``Up movement":\\[0.1ex]  

For all active neurons $C$:\\[1ex]

{\scriptsize $y^{t+1}_{1,C},...,y^{t+1}_{n,C} = f_C (x^{t+1}_{1,C},...,x^{t+1}_{m,C})$.}
}

\end{column}
\end{columns}
}


\end{frame}

\begin{frame}

  \frametitle{Dataflow matrix machines}

Countable network with finite active part at any moment of time.\\[2ex]

Countable matrix {\bf W} with finite number of non-zero elements at any moment of time.


\begin{columns}[T]
\begin{column}{0.7\textwidth}
\begin{tikzpicture}
   \clip (-3.0, -2.0) rectangle (5.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
  \filldraw (-3.2,0) circle [radius=0.5pt]
                (-3.0,0)  circle [radius=0.5pt]
                (-3.4, 0) circle [radius=0.5pt]; 

   \draw [->] (-1.2, 0) -- (-0.8, 1) node [right] {$y_{2,C_1}$};
   \draw [->] (-1.2, 0) -- (-1.6, 1) node [left] {$y_{1,C_1}$};
   \draw (-2.0, -1)  node [left] {$x_{1,C_1}$} -- (-1.2, 0);
   \draw (-1.2, -1)  node [below] {$x_{2,C_1}$} -- (-1.2, 0) node [left] {$f_{C_1}$};
   \draw (-0.4, -1)  node [right] {$x_{3,C_1}$} -- (-1.2, 0);


  \filldraw (0,0) circle [radius=0.5pt]
                (-0.2,0)  circle [radius=0.5pt]
                (0.2, 0) circle [radius=0.5pt];


   \draw (1.6, -1) node [left] {$x_{1,C_2}$} -- (2.0, 0)  node [left] {$f_{C_2}$};;
   \draw (2.4, -1) node [right] {$x_{2,C_2}$} -- (2.0, 0);
   \draw [->] (2.0, 0) -- (1.2, 1) node [left] {$y_{1,C_2}$};
   \draw [->] (2.0, 0) -- (2.0, 1) node [above] {$y_{2,C_2}$};
   \draw [->] (2.0, 0) -- (2.8, 1) node [right] {$y_{3,C_2}$};


  \filldraw (3.2,0) circle [radius=0.5pt]
                (3.0,0)  circle [radius=0.5pt]
                (3.4, 0) circle [radius=0.5pt]; 


  \draw [->, very thick] (0.5, 1.2) .. controls (5.5, 4.2) and (5.5, -4.2) .. (0.5, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.3\textwidth}

\msgray{
\tiny
``Down movement": 

For all inputs $x_{i,C_k}$ where there is a non-zero weight $w_{(i,C_k), (j,C_l)}^t$:\\[1ex]

{\scriptsize$x_{i,C_k}^{t+1} = \sum_{\{(j,C_l) | w_{(i,C_k), (j,C_l)}^t \neq 0\}}$\\[0.1ex]$w_{(i,C_k), (j,C_l)}^t * y_{j, C_l}^{t}.$}\\[3ex]

``Up movement":\\[0.1ex]  

For all active neurons $C$:\\[1ex]

{\scriptsize $y^{t+1}_{1,C},...,y^{t+1}_{n,C} = f_C (x^{t+1}_{1,C},...,x^{t+1}_{m,C})$.}
}

\end{column}
\end{columns}



\end{frame}

\begin{frame}

  \frametitle{Dataflow matrix machines}

Countable network with finite active part at any moment of time.\\[2ex]

Countable matrix {\bf W} with finite number of non-zero elements at any moment of time.


\begin{columns}[T]
\begin{column}{0.7\textwidth}
\begin{tikzpicture}
   \clip (-3.0, -2.0) rectangle (5.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
  \filldraw (-3.2,0) circle [radius=0.5pt]
                (-3.0,0)  circle [radius=0.5pt]
                (-3.4, 0) circle [radius=0.5pt]; 

   \draw [->] (-1.2, 0) -- (-0.8, 1) node [right] {$y_{2,C_1}$};
   \draw [->] (-1.2, 0) -- (-1.6, 1) node [left] {$y_{1,C_1}$};
   \draw (-2.0, -1)  node [left] {$x_{1,C_1}$} -- (-1.2, 0);
   \draw (-1.2, -1)  node [below] {$x_{2,C_1}$} -- (-1.2, 0) node [left] {$f_{C_1}$};
   \draw (-0.4, -1)  node [right] {$x_{3,C_1}$} -- (-1.2, 0);


  \filldraw (0,0) circle [radius=0.5pt]
                (-0.2,0)  circle [radius=0.5pt]
                (0.2, 0) circle [radius=0.5pt];


   \draw (1.6, -1) node [left] {$x_{1,C_2}$} -- (2.0, 0)  node [left] {$f_{C_2}$};;
   \draw (2.4, -1) node [right] {$x_{2,C_2}$} -- (2.0, 0);
   \draw [->] (2.0, 0) -- (1.2, 1) node [left] {$y_{1,C_2}$};
   \draw [->] (2.0, 0) -- (2.0, 1) node [above] {$y_{2,C_2}$};
   \draw [->] (2.0, 0) -- (2.8, 1) node [right] {$y_{3,C_2}$};


  \filldraw (3.2,0) circle [radius=0.5pt]
                (3.0,0)  circle [radius=0.5pt]
                (3.4, 0) circle [radius=0.5pt]; 


  \draw [->, very thick] (0.5, 1.2) .. controls (5.5, 4.2) and (5.5, -4.2) .. (0.5, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.3\textwidth}


\tiny
``Down movement": 

For all inputs $x_{i,C_k}$ where there is a non-zero weight $w_{(i,C_k), (j,C_l)}^t$:\\[1ex]

{\scriptsize$x_{i,C_k}^{t+1} = \sum_{\{(j,C_l) | w_{(i,C_k), (j,C_l)}^t \neq 0\}}$\\[0.1ex]$w_{(i,C_k), (j,C_l)}^t * y_{j, C_l}^{t}.$}\\[3ex]

\msgray{``Up movement":\\[0.1ex]  

For all active neurons $C$:\\[1ex]

{\scriptsize $y^{t+1}_{1,C},...,y^{t+1}_{n,C} = f_C (x^{t+1}_{1,C},...,x^{t+1}_{m,C})$.}
}

\end{column}
\end{columns}



\end{frame}



\begin{frame}

  \frametitle{Dataflow matrix machines}

Countable network with finite active part at any moment of time.\\[2ex]

Countable matrix {\bf W} with finite number of non-zero elements at any moment of time.

\begin{columns}[T]
\begin{column}{0.7\textwidth}
\begin{tikzpicture}
   \clip (-3.0, -2.0) rectangle (5.0, 2.0);
   %\draw (-2.0, -2.0) rectangle (4.0, 2.0);
  \filldraw (-3.2,0) circle [radius=0.5pt]
                (-3.0,0)  circle [radius=0.5pt]
                (-3.4, 0) circle [radius=0.5pt]; 

   \draw [->] (-1.2, 0) -- (-0.8, 1) node [right] {$y_{2,C_1}$};
   \draw [->] (-1.2, 0) -- (-1.6, 1) node [left] {$y_{1,C_1}$};
   \draw (-2.0, -1)  node [left] {$x_{1,C_1}$} -- (-1.2, 0);
   \draw (-1.2, -1)  node [below] {$x_{2,C_1}$} -- (-1.2, 0) node [left] {$f_{C_1}$};
   \draw (-0.4, -1)  node [right] {$x_{3,C_1}$} -- (-1.2, 0);


  \filldraw (0,0) circle [radius=0.5pt]
                (-0.2,0)  circle [radius=0.5pt]
                (0.2, 0) circle [radius=0.5pt];


   \draw (1.6, -1) node [left] {$x_{1,C_2}$} -- (2.0, 0)  node [left] {$f_{C_2}$};;
   \draw (2.4, -1) node [right] {$x_{2,C_2}$} -- (2.0, 0);
   \draw [->] (2.0, 0) -- (1.2, 1) node [left] {$y_{1,C_2}$};
   \draw [->] (2.0, 0) -- (2.0, 1) node [above] {$y_{2,C_2}$};
   \draw [->] (2.0, 0) -- (2.8, 1) node [right] {$y_{3,C_2}$};


  \filldraw (3.2,0) circle [radius=0.5pt]
                (3.0,0)  circle [radius=0.5pt]
                (3.4, 0) circle [radius=0.5pt]; 


  \draw [->, very thick] (0.5, 1.2) .. controls (5.5, 4.2) and (5.5, -4.2) .. (0.5, -1.2)  node [midway, right] {{\bf W}};

\end{tikzpicture}

\end{column}
\begin{column}{0.3\textwidth}

{%\scriptsize
\tiny
``Down movement": 

For all inputs $x_{i,C_k}$ where there is a non-zero weight $w_{(i,C_k), (j,C_l)}^t$:\\[1ex]

{\scriptsize$x_{i,C_k}^{t+1} = \sum_{\{(j,C_l) | w_{(i,C_k), (j,C_l)}^t \neq 0\}}$\\[0.1ex]$w_{(i,C_k), (j,C_l)}^t * y_{j, C_l}^{t}.$}\\[3ex]

``Up movement":\\[0.1ex]  

For all active neurons $C$:\\[1ex]

{\scriptsize $y^{t+1}_{1,C},...,y^{t+1}_{n,C} = f_C (x^{t+1}_{1,C},...,x^{t+1}_{m,C})$.}
}

\end{column}
\end{columns}


\end{frame}





\begin{frame}

  \frametitle{Type correctness condition}

We need to keep input and output arity of each neuron in mind, and we also need to impose a type correctness condition in order for the formulas in the previous slide to be well-defined:\\[2ex]


$w_{(i,C_k), (j,C_l)}^t$ is allowed to be
non-zero only if $x_{i,C_k}$ and $y_{j, C_l}$ belong to the same {\em kind} of linear streams.\\[4ex]

\msgray{We are now going to introduce linear streams of {\em V-values} based on {\em nested dictionaries}, which are sufficiently universal and expressive to save us from the need to impose type correctness conditions.\\[2ex]

Moreover, they will allow us to define {\em variadic neurons}, so that we don't need to keep track on input and output
arity either.}

\end{frame}

\begin{frame}

  \frametitle{Type correctness condition}

We need to keep input and output arity of each neuron in mind, and we also need to impose a type correctness condition in order for the formulas in the previous slide to be well-defined:\\[2ex]


$w_{(i,C_k), (j,C_l)}^t$ is allowed to be
non-zero only if $x_{i,C_k}$ and $y_{j, C_l}$ belong to the same {\em kind} of linear streams.\\[4ex]

We are now going to introduce linear streams of {\em V-values} based on {\em nested dictionaries}, which are sufficiently universal and expressive to save us from the need to impose type correctness conditions.\\[2ex]

\msgray{Moreover, they will allow us to define {\em variadic neurons}, so that we don't need to keep track on input and output
arity either.}

\end{frame}

\begin{frame}

  \frametitle{Type correctness condition}

We need to keep input and output arity of each neuron in mind, and we also need to impose a type correctness condition in order for the formulas in the previous slide to be well-defined:\\[2ex]


$w_{(i,C_k), (j,C_l)}^t$ is allowed to be
non-zero only if $x_{i,C_k}$ and $y_{j, C_l}$ belong to the same {\em kind} of linear streams.\\[4ex]

We are now going to introduce linear streams of {\em V-values} based on {\em nested dictionaries}, which are sufficiently universal and expressive to save us from the need to impose type correctness conditions.\\[2ex]

Moreover, they will allow us to define {\em variadic neurons}, so that we don't need to keep track on input and output
arity either.

\end{frame}






\section{V-values and variadic neurons}

\begin{frame}

  \frametitle{Vector space based on nested dictionaries}

Lisp introduced nested lists (S-expressions) in 1958.\\[2ex]

\msgray{It might be the case when computational resources are sufficient, 
that if one has to base a formalism on a single kind of nested data structure, this structure should be nested dictionaries.\\[2ex]

The simplest way to build a vector space of nested dictionaries is to require
all atoms (leaves) to be numbers.\\[2ex]

We call nested dictionaries with numerical atoms {\em V-values}.\\[2ex]

(A more general construction of V-values involving nested dictionaries with more complicated atoms is considered in the companion paper for this talk.)}

\end{frame}

\begin{frame}

  \frametitle{Vector space based on nested dictionaries}

Lisp introduced nested lists (S-expressions) in 1958.\\[2ex]

It might be the case when computational resources are sufficient, 
that if one has to base a formalism on a single kind of nested data structure, this structure should be nested dictionaries.\\[2ex]

\msgray{The simplest way to build a vector space of nested dictionaries is to require
all atoms (leaves) to be numbers.\\[2ex]

We call nested dictionaries with numerical atoms {\em V-values}.\\[2ex]

(A more general construction of V-values involving nested dictionaries with more complicated atoms is considered in the companion paper for this talk.)}

\end{frame}

\begin{frame}

  \frametitle{Vector space based on nested dictionaries}

Lisp introduced nested lists (S-expressions) in 1958.\\[2ex]

It might be the case when computational resources are sufficient, 
that if one has to base a formalism on a single kind of nested data structure, this structure should be nested dictionaries.\\[2ex]

The simplest way to build a vector space of nested dictionaries is to require
all atoms (leaves) to be numbers.\\[2ex]

\msgray{We call nested dictionaries with numerical atoms {\em V-values}.\\[2ex]

(A more general construction of V-values involving nested dictionaries with more complicated atoms is considered in the companion paper for this talk.)}

\end{frame}


\begin{frame}

  \frametitle{Vector space based on nested dictionaries}

Lisp introduced nested lists (S-expressions) in 1958.\\[2ex]

It might be the case when computational resources are sufficient, 
that if one has to base a formalism on a single kind of nested data structure, this structure should be nested dictionaries.\\[2ex]

The simplest way to build a vector space of nested dictionaries is to require
all atoms (leaves) to be numbers.\\[2ex]

We call nested dictionaries with numerical atoms {\em V-values}.\\[2ex]

\msgray{(A more general construction of V-values involving nested dictionaries with more complicated atoms is considered in the companion paper for this talk.)}

\end{frame}

\begin{frame}

  \frametitle{Vector space based on nested dictionaries}

Lisp introduced nested lists (S-expressions) in 1958.\\[2ex]

It might be the case when computational resources are sufficient, 
that if one has to base a formalism on a single kind of nested data structure, this structure should be nested dictionaries.\\[2ex]

The simplest way to build a vector space of nested dictionaries is to require
all atoms (leaves) to be numbers.\\[2ex]

We call nested dictionaries with numerical atoms {\em V-values}.\\[2ex]

(A more general construction of V-values involving nested dictionaries with more complicated atoms is considered in the companion paper for this talk.)

\end{frame}



\begin{frame}

  \frametitle{Ways to understand V-values}

Consider ordinary words (``labels") to be letters of a countable ``super-alphabet" $L$.\\[2ex]

\msgray{V-values can be understood as}

\begin{itemize}

\msgray{
  \item Finite linear combinations of finite strings of letters from $L$;
  \item Finite prefix trees with numerical leaves;
  \item Sparse ``tensors of mixed rank" with finite number of non-zero elements;
  \item Recurrent maps (that is, nested dictionaries) from $V \cong \mathbb{R}\oplus (L \rightarrow V)$  admitting finite descriptions.
}

\end{itemize}


\end{frame}

\begin{frame}

  \frametitle{Ways to understand V-values}

Consider ordinary words (``labels") to be letters of a countable ``super-alphabet" $L$.\\[2ex]

V-values can be understood as

\begin{itemize}


  \item Finite linear combinations of finite strings of letters from $L$;
\msgray{
  \item Finite prefix trees with numerical leaves;
  \item Sparse ``tensors of mixed rank" with finite number of non-zero elements;
  \item Recurrent maps (that is, nested dictionaries) from $V \cong \mathbb{R}\oplus (L \rightarrow V)$  admitting finite descriptions.
}

\end{itemize}


\end{frame}

\begin{frame}

  \frametitle{Ways to understand V-values}

Consider ordinary words (``labels") to be letters of a countable ``super-alphabet" $L$.\\[2ex]

V-values can be understood as

\begin{itemize}


  \item Finite linear combinations of finite strings of letters from $L$;
  \item Finite prefix trees with numerical leaves;
\msgray{ 
 \item Sparse ``tensors of mixed rank" with finite number of non-zero elements;
  \item Recurrent maps (that is, nested dictionaries) from $V \cong \mathbb{R}\oplus (L \rightarrow V)$  admitting finite descriptions.
}

\end{itemize}


\end{frame}






\begin{frame}

  \frametitle{Ways to understand V-values}

Consider ordinary words (``labels") to be letters of a countable ``super-alphabet" $L$.\\[2ex]

V-values can be understood as

\begin{itemize}


  \item Finite linear combinations of finite strings of letters from $L$;
  \item Finite prefix trees with numerical leaves;
  \item Sparse ``tensors of mixed rank" with finite number of non-zero elements;
\msgray{
  \item Recurrent maps (that is, nested dictionaries) from $V \cong \mathbb{R}\oplus (L \rightarrow V)$  admitting finite descriptions.
}

\end{itemize}


\end{frame}


\begin{frame}

  \frametitle{Ways to understand V-values}

Consider ordinary words (``labels") to be letters of a countable ``super-alphabet" $L$.\\[2ex]

V-values can be understood as

\begin{itemize}

  \item Finite linear combinations of finite strings of letters from $L$;
  \item Finite prefix trees with numerical leaves;
  \item Sparse ``tensors of mixed rank" with finite number of non-zero elements;
  \item Recurrent maps (that is, nested dictionaries) from $V \cong \mathbb{R}\oplus (L \rightarrow V)$  admitting finite descriptions.

\end{itemize}

\end{frame}





\begin{frame}

  \frametitle{Example of a V-value}

\begin{columns}[T]
\begin{column}{0.5\textwidth}

\begin{tikzpicture}
  \clip (-3.0, -4.5) rectangle (3.0, 0.0);

  \draw[->](0, 0) -- (-1.8, -1)  node [below] {3.5};

  \draw[->](0, 0) -- (0, -1) node [midway,  below right] {:foo};

    \draw[->](0, -1) -- (0.5, -2) node[midway, below right] {\ :bar};

      \draw[->](0.5, -2) -- (0.5, -3) node [below] {7};

    \draw[->](0, -1) -- (-0.5, -2) node [below] {2};

  \draw[->](0, 0) -- (1.8, -1) node [midway, below  right] {\ \ \ \ \ \ :baz};

      \draw[->](1.8, -1) -- (1.8, -2) node [midway, below  right] {:foo};

      \draw[->](1.8, -2) -- (1.8, -3) node [midway, below  right] {:bar};      

      \draw[->](1.8, -3) -- (1.8, -4) node [below] {-4};
 

  \filldraw (1.8, -1) circle [radius=0.7pt]
                (1.8, -2) circle [radius=0.7pt]
                (1.8, -3) circle [radius=0.7pt]
                (0, -1) circle [radius=0.7pt]
                (0.5, -2) circle [radius=0.7pt];
  
 

\end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\begin{itemize}

{\footnotesize

\msgray{
 \item 3.5 $\cdot$ ($\epsilon$) + 2 $\cdot$ (:foo) + \\ 7 $\cdot$ (:foo :bar) - 4 $\cdot$ (:baz :foo :bar)

 \item ($\leadsto$ 3.5) + (:foo $\leadsto$  2) +\\ (:foo $\leadsto$ :bar $\leadsto$ 7) +\\
          (:baz $\leadsto$ :foo $\leadsto$ :bar $\leadsto$ -4)

 \item {\tiny  scalar 3.5 + sparse 1D array {\tt d1[:foo] = 2} +\\[1.5ex] sparse 2D
matrix {\tt d2[:foo, :bar] = 7} +\\  sparse 3D array {\tt d3[:baz, :foo, :bar] = -4}}

 \item {\tt \{:number 3.5,\\ :foo \{:number 2, :bar 7\}, :baz \{:foo \{:bar -4\}\}\}}
          ({\tt :number} $\not\in L$)
}
}

\end{itemize}

\end{column}
\end{columns}

\end{frame}

\begin{frame}

  \frametitle{Example of a V-value}

\begin{columns}[T]
\begin{column}{0.5\textwidth}

\begin{tikzpicture}
  \clip (-3.0, -4.5) rectangle (3.0, 0.0);

  \draw[->](0, 0) -- (-1.8, -1)  node [below] {3.5};

  \draw[->](0, 0) -- (0, -1) node [midway,  below right] {:foo};

    \draw[->](0, -1) -- (0.5, -2) node[midway, below right] {\ :bar};

      \draw[->](0.5, -2) -- (0.5, -3) node [below] {7};

    \draw[->](0, -1) -- (-0.5, -2) node [below] {2};

  \draw[->](0, 0) -- (1.8, -1) node [midway, below  right] {\ \ \ \ \ \ :baz};

      \draw[->](1.8, -1) -- (1.8, -2) node [midway, below  right] {:foo};

      \draw[->](1.8, -2) -- (1.8, -3) node [midway, below  right] {:bar};      

      \draw[->](1.8, -3) -- (1.8, -4) node [below] {-4};
 

  \filldraw (1.8, -1) circle [radius=0.7pt]
                (1.8, -2) circle [radius=0.7pt]
                (1.8, -3) circle [radius=0.7pt]
                (0, -1) circle [radius=0.7pt]
                (0.5, -2) circle [radius=0.7pt];
  
 

\end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\begin{itemize}

{\footnotesize

 \item 3.5 $\cdot$ ($\epsilon$) + 2 $\cdot$ (:foo) + \\ 7 $\cdot$ (:foo :bar) - 4 $\cdot$ (:baz :foo :bar)

\msgray{
 \item ($\leadsto$ 3.5) + (:foo $\leadsto$  2) +\\ (:foo $\leadsto$ :bar $\leadsto$ 7) +\\
          (:baz $\leadsto$ :foo $\leadsto$ :bar $\leadsto$ -4)

 \item {\tiny  scalar 3.5 + sparse 1D array {\tt d1[:foo] = 2} +\\[1.5ex] sparse 2D
matrix {\tt d2[:foo, :bar] = 7} +\\  sparse 3D array {\tt d3[:baz, :foo, :bar] = -4}}

 \item {\tt \{:number 3.5,\\ :foo \{:number 2, :bar 7\}, :baz \{:foo \{:bar -4\}\}\}}
          ({\tt :number} $\not\in L$)
}
}

\end{itemize}

\end{column}
\end{columns}

\end{frame}

\begin{frame}

  \frametitle{Example of a V-value}

\begin{columns}[T]
\begin{column}{0.5\textwidth}

\begin{tikzpicture}
  \clip (-3.0, -4.5) rectangle (3.0, 0.0);

  \draw[->](0, 0) -- (-1.8, -1)  node [below] {3.5};

  \draw[->](0, 0) -- (0, -1) node [midway,  below right] {:foo};

    \draw[->](0, -1) -- (0.5, -2) node[midway, below right] {\ :bar};

      \draw[->](0.5, -2) -- (0.5, -3) node [below] {7};

    \draw[->](0, -1) -- (-0.5, -2) node [below] {2};

  \draw[->](0, 0) -- (1.8, -1) node [midway, below  right] {\ \ \ \ \ \ :baz};

      \draw[->](1.8, -1) -- (1.8, -2) node [midway, below  right] {:foo};

      \draw[->](1.8, -2) -- (1.8, -3) node [midway, below  right] {:bar};      

      \draw[->](1.8, -3) -- (1.8, -4) node [below] {-4};
 

  \filldraw (1.8, -1) circle [radius=0.7pt]
                (1.8, -2) circle [radius=0.7pt]
                (1.8, -3) circle [radius=0.7pt]
                (0, -1) circle [radius=0.7pt]
                (0.5, -2) circle [radius=0.7pt];
  
 

\end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\begin{itemize}

{\footnotesize

 \item 3.5 $\cdot$ ($\epsilon$) + 2 $\cdot$ (:foo) + \\ 7 $\cdot$ (:foo :bar) - 4 $\cdot$ (:baz :foo :bar)

 \item ($\leadsto$ 3.5) + (:foo $\leadsto$  2) +\\ (:foo $\leadsto$ :bar $\leadsto$ 7) +\\
          (:baz $\leadsto$ :foo $\leadsto$ :bar $\leadsto$ -4)

\msgray{
 \item {\tiny  scalar 3.5 + sparse 1D array {\tt d1[:foo] = 2} +\\[1.5ex] sparse 2D
matrix {\tt d2[:foo, :bar] = 7} +\\  sparse 3D array {\tt d3[:baz, :foo, :bar] = -4}}

 \item {\tt \{:number 3.5,\\ :foo \{:number 2, :bar 7\}, :baz \{:foo \{:bar -4\}\}\}}
          ({\tt :number} $\not\in L$)
}
}

\end{itemize}

\end{column}
\end{columns}

\end{frame}

\begin{frame}

  \frametitle{Example of a V-value}

\begin{columns}[T]
\begin{column}{0.5\textwidth}

\begin{tikzpicture}
  \clip (-3.0, -4.5) rectangle (3.0, 0.0);

  \draw[->](0, 0) -- (-1.8, -1)  node [below] {3.5};

  \draw[->](0, 0) -- (0, -1) node [midway,  below right] {:foo};

    \draw[->](0, -1) -- (0.5, -2) node[midway, below right] {\ :bar};

      \draw[->](0.5, -2) -- (0.5, -3) node [below] {7};

    \draw[->](0, -1) -- (-0.5, -2) node [below] {2};

  \draw[->](0, 0) -- (1.8, -1) node [midway, below  right] {\ \ \ \ \ \ :baz};

      \draw[->](1.8, -1) -- (1.8, -2) node [midway, below  right] {:foo};

      \draw[->](1.8, -2) -- (1.8, -3) node [midway, below  right] {:bar};      

      \draw[->](1.8, -3) -- (1.8, -4) node [below] {-4};
 

  \filldraw (1.8, -1) circle [radius=0.7pt]
                (1.8, -2) circle [radius=0.7pt]
                (1.8, -3) circle [radius=0.7pt]
                (0, -1) circle [radius=0.7pt]
                (0.5, -2) circle [radius=0.7pt];
  
 

\end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\begin{itemize}

{\footnotesize

 \item 3.5 $\cdot$ ($\epsilon$) + 2 $\cdot$ (:foo) + \\ 7 $\cdot$ (:foo :bar) - 4 $\cdot$ (:baz :foo :bar)

 \item ($\leadsto$ 3.5) + (:foo $\leadsto$  2) +\\ (:foo $\leadsto$ :bar $\leadsto$ 7) +\\
          (:baz $\leadsto$ :foo $\leadsto$ :bar $\leadsto$ -4)

 \item {\tiny  scalar 3.5 + sparse 1D array {\tt d1[:foo] = 2} +\\[1.5ex] sparse 2D
matrix {\tt d2[:foo, :bar] = 7} +\\  sparse 3D array {\tt d3[:baz, :foo, :bar] = -4}}

\msgray{
 \item {\tt \{:number 3.5,\\ :foo \{:number 2, :bar 7\}, :baz \{:foo \{:bar -4\}\}\}}
          ({\tt :number} $\not\in L$)
}
}

\end{itemize}

\end{column}
\end{columns}

\end{frame}

\begin{frame}

  \frametitle{Example of a V-value}

\begin{columns}[T]
\begin{column}{0.5\textwidth}

\begin{tikzpicture}
  \clip (-3.0, -4.5) rectangle (3.0, 0.0);

  \draw[->](0, 0) -- (-1.8, -1)  node [below] {3.5};

  \draw[->](0, 0) -- (0, -1) node [midway,  below right] {:foo};

    \draw[->](0, -1) -- (0.5, -2) node[midway, below right] {\ :bar};

      \draw[->](0.5, -2) -- (0.5, -3) node [below] {7};

    \draw[->](0, -1) -- (-0.5, -2) node [below] {2};

  \draw[->](0, 0) -- (1.8, -1) node [midway, below  right] {\ \ \ \ \ \ :baz};

      \draw[->](1.8, -1) -- (1.8, -2) node [midway, below  right] {:foo};

      \draw[->](1.8, -2) -- (1.8, -3) node [midway, below  right] {:bar};      

      \draw[->](1.8, -3) -- (1.8, -4) node [below] {-4};
 

  \filldraw (1.8, -1) circle [radius=0.7pt]
                (1.8, -2) circle [radius=0.7pt]
                (1.8, -3) circle [radius=0.7pt]
                (0, -1) circle [radius=0.7pt]
                (0.5, -2) circle [radius=0.7pt];
  
 

\end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\begin{itemize}

{\footnotesize

 \item 3.5 $\cdot$ ($\epsilon$) + 2 $\cdot$ (:foo) + \\ 7 $\cdot$ (:foo :bar) - 4 $\cdot$ (:baz :foo :bar)

 \item ($\leadsto$ 3.5) + (:foo $\leadsto$  2) +\\ (:foo $\leadsto$ :bar $\leadsto$ 7) +\\
          (:baz $\leadsto$ :foo $\leadsto$ :bar $\leadsto$ -4)

 \item {\tiny  scalar 3.5 + sparse 1D array {\tt d1[:foo] = 2} +\\[1.5ex] sparse 2D
matrix {\tt d2[:foo, :bar] = 7} +\\  sparse 3D array {\tt d3[:baz, :foo, :bar] = -4}}

 \item {\tt \{:number 3.5,\\ :foo \{:number 2, :bar 7\}, :baz \{:foo \{:bar -4\}\}\}}
\msgray{
          ({\tt :number} $\not\in L$)
}
}

\end{itemize}

\end{column}
\end{columns}

\end{frame}


\begin{frame}

  \frametitle{Example of a V-value}

\begin{columns}[T]
\begin{column}{0.5\textwidth}

\begin{tikzpicture}
  \clip (-3.0, -4.5) rectangle (3.0, 0.0);

  \draw[->](0, 0) -- (-1.8, -1)  node [below] {3.5};

  \draw[->](0, 0) -- (0, -1) node [midway,  below right] {:foo};

    \draw[->](0, -1) -- (0.5, -2) node[midway, below right] {\ :bar};

      \draw[->](0.5, -2) -- (0.5, -3) node [below] {7};

    \draw[->](0, -1) -- (-0.5, -2) node [below] {2};

  \draw[->](0, 0) -- (1.8, -1) node [midway, below  right] {\ \ \ \ \ \ :baz};

      \draw[->](1.8, -1) -- (1.8, -2) node [midway, below  right] {:foo};

      \draw[->](1.8, -2) -- (1.8, -3) node [midway, below  right] {:bar};      

      \draw[->](1.8, -3) -- (1.8, -4) node [below] {-4};
 

  \filldraw (1.8, -1) circle [radius=0.7pt]
                (1.8, -2) circle [radius=0.7pt]
                (1.8, -3) circle [radius=0.7pt]
                (0, -1) circle [radius=0.7pt]
                (0.5, -2) circle [radius=0.7pt];
  
 

\end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\begin{itemize}

{\footnotesize

 \item 3.5 $\cdot$ ($\epsilon$) + 2 $\cdot$ (:foo) + \\ 7 $\cdot$ (:foo :bar) - 4 $\cdot$ (:baz :foo :bar)

 \item ($\leadsto$ 3.5) + (:foo $\leadsto$  2) +\\ (:foo $\leadsto$ :bar $\leadsto$ 7) +\\
          (:baz $\leadsto$ :foo $\leadsto$ :bar $\leadsto$ -4)

 \item {\tiny  scalar 3.5 + sparse 1D array {\tt d1[:foo] = 2} +\\[1.5ex] sparse 2D
matrix {\tt d2[:foo, :bar] = 7} +\\  sparse 3D array {\tt d3[:baz, :foo, :bar] = -4}}

 \item {\tt \{:number 3.5,\\ :foo \{:number 2, :bar 7\}, :baz \{:foo \{:bar -4\}\}\}}
\msdark{ % for better formatting
          ({\tt :number} $\not\in L$)
}

}

\end{itemize}

\end{column}
\end{columns}

\end{frame}







\begin{frame}

  \frametitle{Dataflow matrix machines\\  based on
 streams of  V-values
 and
 variadic neurons}

$x_{f, n_f, i}^{t+1} = \sum_{g \in F} \sum_{n_g \in L} \sum_{o \in L} w_{f, n_f, i;\, g, n_g, o}^t * y_{g, n_g, o}^t$ {\small (down movement)}\\[2ex]

$y_{f, n_f}^{t+1} = f(x_{f, n_f}^{t+1})$ {\small (up movement)}

\begin{tikzpicture}
  %\draw (-3.5, -2.0) rectangle (7.0, 2.0);
  \clip (-3.5, -2.0) rectangle (7.0, 2.0);

  \filldraw (-3.2,0) circle [radius=0.5pt]
                (-3.0,0)  circle [radius=0.5pt]
                (-3.4, 0) circle [radius=0.5pt]; 

  \draw [->] (-2.6, -1.5) node[right] {$x_{f, n_f}$} -- (-2.6, 1.5) node [midway, above right] {$f$} node[right] {$y_{f, n_f}$};

  \filldraw (0,0) circle [radius=0.5pt]
                (-0.2,0)  circle [radius=0.5pt]
                (0.2, 0) circle [radius=0.5pt];

  \draw [->] (0.6, -1.5) node[right] {$x_{g, n_g}$} -- (0.6, 1.5) node [midway, above right] {$g$} node[right] {$y_{g, n_g}$};

  \filldraw (3.2,0) circle [radius=0.5pt]
                (3.0,0)  circle [radius=0.5pt]
                (3.4, 0) circle [radius=0.5pt]; 


  \draw [->, very thick] (1.1, 1.1) .. controls (5.5, 4.3) and (5.5, -4.0) .. (1.1, -0.8)  node [midway, right] {{\bf W}};

  \foreach \y in {-1.0, 1.0}
    {

      \draw [densely dotted] (0.45, \y+0.15) ellipse [x radius=100pt, y radius=4pt];

     \foreach \x in {-1.0, 2.2}
       {

        \foreach \d in {-0.4, -0.15, 0.1}
           {
               \draw(\x-0.15, \y + 0.45) -- (\x+\d, \y+0.15); 
               \draw (\x+\d, \y + 0.15) -- (\x+\d-0.08, \y-0.15);
               \draw (\x+\d, \y + 0.15) -- (\x+\d+0.08, \y-0.15);
               \filldraw (\x+0.5*\d-0.05, \y-0.25) circle [radius=0.2pt];
               \filldraw (\x+0.5*\d+0.5, \y+0.15) circle [radius=0.2pt];
           }
       } 
     }
 
\end{tikzpicture}

\end{frame}

\section{Programming patterns and self-referential facilities}



\begin{frame}

  \frametitle{Powerful neurons}

With powerful variadic neurons and streams of V-values we have a formalism which is much more
expressive than neural nets based on streams of numbers.\\[2ex]

\msgray{Many tasks can be accomplished by compact networks, with single neurons functioning as layers or modules.\\[2ex]

We only touch this topic lightly here; see the companion paper for more...}

\end{frame}

\begin{frame}

  \frametitle{Powerful neurons}

With powerful variadic neurons and streams of V-values we have a formalism which is much more
expressive than neural nets based on streams of numbers.\\[2ex]

Many tasks can be accomplished by compact networks, with single neurons functioning as layers or modules.\\[2ex]

\msgray{We only touch this topic lightly here; see the companion paper for more...}

\end{frame}

\begin{frame}

  \frametitle{Powerful neurons}

With powerful variadic neurons and streams of V-values we have a formalism which is much more
expressive than neural nets based on streams of numbers.\\[2ex]

Many tasks can be accomplished by compact networks, with single neurons functioning as layers or modules.\\[2ex]

We only touch this topic lightly here; see the companion paper for more...

\end{frame}


\subsection{Sparse vectors}

\begin{frame}

  \frametitle{Sparse vectors of high or infinite dimension}

Consider a neuron accumulating count of words in a given text.\\[2ex]

\msgray{The dictionary mapping words to their respective counts is an infinite-dimensional vector
with a finite number of non-zero elements.\\[2ex]

Don't need a neuron for each coordinate of our vector space\\[2ex]

Don't have an obligation to reduce dimension by embedding.}

\end{frame}

\begin{frame}

  \frametitle{Sparse vectors of high or infinite dimension}

Consider a neuron accumulating count of words in a given text.\\[2ex]

The dictionary mapping words to their respective counts is an infinite-dimensional vector
with a finite number of non-zero elements.\\[2ex]

\msgray{Don't need a neuron for each coordinate of our vector space\\[2ex]

Don't have an obligation to reduce dimension by embedding.}

\end{frame}

\begin{frame}

  \frametitle{Sparse vectors of high or infinite dimension}

Consider a neuron accumulating count of words in a given text.\\[2ex]

The dictionary mapping words to their respective counts is an infinite-dimensional vector
with a finite number of non-zero elements.\\[2ex]

Don't need a neuron for each coordinate of our vector space\\[2ex]

\msgray{Don't have an obligation to reduce dimension by embedding.}

\end{frame}

\begin{frame}

  \frametitle{Sparse vectors of high or infinite dimension}

Consider a neuron accumulating count of words in a given text.\\[2ex]

The dictionary mapping words to their respective counts is an infinite-dimensional vector
with a finite number of non-zero elements.\\[2ex]

Don't need a neuron for each coordinate of our vector space\\[2ex]

Don't have an obligation to reduce dimension by embedding.

\end{frame}





\subsection{Data structures}



\begin{frame}

  \frametitle{Streams of immutable data structures}

It is convenient to represent a large variety of data structures via nested dictionaries. One can represent lists, matrices, graphs, and so on in this fashion.\\[1ex]

\msgray{The computational architecture promoted by the functional programming community is to work with immutable data sharing common substructures and garbage collecting the obsolete parts.\\[1ex]

The experience of the field shows that this approach is pretty efficient in general and allows easy parallelization.\\[1ex]

It is natural to use streams of immutable V-values in the implementations of DMMs.\\[1ex]

Hence, the DMM architecture is friendly towards implementing algorithms working with immutable data structures in
the spirit of functional programming.}

\end{frame}

\begin{frame}

  \frametitle{Streams of immutable data structures}

It is convenient to represent a large variety of data structures via nested dictionaries. One can represent lists, matrices, graphs, and so on in this fashion.\\[1ex]

The computational architecture promoted by the functional programming community is to work with immutable data sharing common substructures and garbage collecting the obsolete parts.\\[1ex]

\msgray{The experience of the field shows that this approach is pretty efficient in general and allows easy parallelization.\\[1ex]

It is natural to use streams of immutable V-values in the implementations of DMMs.\\[1ex]

Hence, the DMM architecture is friendly towards implementing algorithms working with immutable data structures in
the spirit of functional programming.}

\end{frame}

\begin{frame}

  \frametitle{Streams of immutable data structures}

It is convenient to represent a large variety of data structures via nested dictionaries. One can represent lists, matrices, graphs, and so on in this fashion.\\[1ex]

The computational architecture promoted by the functional programming community is to work with immutable data sharing common substructures and garbage collecting the obsolete parts.\\[1ex]

The experience of the field shows that this approach is pretty efficient in general and allows easy parallelization.\\[1ex]

\msgray{It is natural to use streams of immutable V-values in the implementations of DMMs.\\[1ex]

Hence, the DMM architecture is friendly towards implementing algorithms working with immutable data structures in
the spirit of functional programming.}

\end{frame}

\begin{frame}

  \frametitle{Streams of immutable data structures}

It is convenient to represent a large variety of data structures via nested dictionaries. One can represent lists, matrices, graphs, and so on in this fashion.\\[1ex]

The computational architecture promoted by the functional programming community is to work with immutable data sharing common substructures and garbage collecting the obsolete parts.\\[1ex]

The experience of the field shows that this approach is pretty efficient in general and allows easy parallelization.\\[1ex]

It is natural to use streams of immutable V-values in the implementations of DMMs.\\[1ex]

\msgray{Hence, the DMM architecture is friendly towards implementing algorithms working with immutable data structures in
the spirit of functional programming.}

\end{frame}

\begin{frame}

  \frametitle{Streams of immutable data structures}

It is convenient to represent a large variety of data structures via nested dictionaries. One can represent lists, matrices, graphs, and so on in this fashion.\\[1ex]

The computational architecture promoted by the functional programming community is to work with immutable data sharing common substructures and garbage collecting the obsolete parts.\\[1ex]

The experience of the field shows that this approach is pretty efficient in general and allows easy parallelization.\\[1ex]

It is natural to use streams of immutable V-values in the implementations of DMMs.\\[1ex]

Hence, the DMM architecture is friendly towards implementing algorithms working with immutable data structures in
the spirit of functional programming.

\end{frame}




\subsection{Self-referential facilities}



\begin{frame}

  \frametitle{Self-referential facilities}

It is easy to represent the network matrix {\bf W} as a V-value.\\[2ex]

\msgray{Therefore, it is easy to designate a particular neuron {\tt Self} and a
particular output of that neuron (say, {\tt :active}) as emitting the streams
of network matrices.\\[2ex]

The V-value at that output of {\tt Self} obtained during the most recent
``up movement" is used as the network matrix {\bf W} during the
next ``down movement".\\[2ex]

This mechanism allows any DMM network to {\bf modify its own topology and weights}
while it is running.}

\end{frame}

\begin{frame}

  \frametitle{Self-referential facilities}

It is easy to represent the network matrix {\bf W} as a V-value.\\[2ex]

Therefore, it is easy to designate a particular neuron {\tt Self} and a
particular output of that neuron (say, {\tt :active}) as emitting the streams
of network matrices.\\[2ex]

\msgray{The V-value at that output of {\tt Self} obtained during the most recent
``up movement" is used as the network matrix {\bf W} during the
next ``down movement".\\[2ex]

This mechanism allows any DMM network to {\bf modify its own topology and weights}
while it is running.}

\end{frame}

\begin{frame}

  \frametitle{Self-referential facilities}

It is easy to represent the network matrix {\bf W} as a V-value.\\[2ex]

Therefore, it is easy to designate a particular neuron {\tt Self} and a
particular output of that neuron (say, {\tt :active}) as emitting the streams
of network matrices.\\[2ex]

The V-value at that output of {\tt Self} obtained during the most recent
``up movement" is used as the network matrix {\bf W} during the
next ``down movement".\\[2ex]

\msgray{This mechanism allows any DMM network to {\bf modify its own topology and weights}
while it is running.}

\end{frame}

\begin{frame}

  \frametitle{Self-referential facilities}

It is easy to represent the network matrix {\bf W} as a V-value.\\[2ex]

Therefore, it is easy to designate a particular neuron {\tt Self} and a
particular output of that neuron (say, {\tt :active}) as emitting the streams
of network matrices.\\[2ex]

The V-value at that output of {\tt Self} obtained during the most recent
``up movement" is used as the network matrix {\bf W} during the
next ``down movement".\\[2ex]

This mechanism allows any DMM network to {\bf modify its own topology and weights}
while it is running.

\end{frame}



\begin{frame}

  \frametitle{Self-referential facilities}

This is something which is theoretically possible, but practically very difficult
to do in neural nets based on streams of numbers.\\[2ex]

\msgray{Our current implementation uses {\tt Self} which works as an accumulator.
It accumulates the value of the network matrix while
accepting additive updates from other neurons in the network.\\[2ex]

In turn, the other neurons can use {\tt Self} outputs to become aware of
the current network structure and weights.\\[2ex]

Our experiments show that this mechanism together with simple
constant update neurons is capable of producing waves of connectivity
patterns propagating within the network matrix {\bf W}.}

\end{frame}

\begin{frame}

  \frametitle{Self-referential facilities}

This is something which is theoretically possible, but practically very difficult
to do in neural nets based on streams of numbers.\\[2ex]

Our current implementation uses {\tt Self} which works as an accumulator.
It accumulates the value of the network matrix while
accepting additive updates from other neurons in the network.\\[2ex]

\msgray{In turn, the other neurons can use {\tt Self} outputs to become aware of
the current network structure and weights.\\[2ex]

Our experiments show that this mechanism together with simple
constant update neurons is capable of producing waves of connectivity
patterns propagating within the network matrix {\bf W}.}

\end{frame}

\begin{frame}

  \frametitle{Self-referential facilities}

This is something which is theoretically possible, but practically very difficult
to do in neural nets based on streams of numbers.\\[2ex]

Our current implementation uses {\tt Self} which works as an accumulator.
It accumulates the value of the network matrix while
accepting additive updates from other neurons in the network.\\[2ex]

In turn, the other neurons can use {\tt Self} outputs to become aware of
the current network structure and weights.\\[2ex]

\msgray{Our experiments show that this mechanism together with simple
constant update neurons is capable of producing waves of connectivity
patterns propagating within the network matrix {\bf W}.}

\end{frame}


\begin{frame}

  \frametitle{Self-referential facilities}

This is something which is theoretically possible, but practically very difficult
to do in neural nets based on streams of numbers.\\[2ex]

Our current implementation uses {\tt Self} which works as an accumulator.
It accumulates the value of the network matrix while
accepting additive updates from other neurons in the network.\\[2ex]

In turn, the other neurons can use {\tt Self} outputs to become aware of
the current network structure and weights.\\[2ex]

Our experiments show that this mechanism together with simple
constant update neurons is capable of producing waves of connectivity
patterns propagating within the network matrix {\bf W}.

\end{frame}

\begin{frame}

  \frametitle{Applications...}

We hope that this formalism will be useful in various\\ {\bf learning to learn} setups\\
(the systems which learn how to learn better),\\ in {\bf program synthesis}, and more...

\end{frame}



\begin{frame}

  \frametitle{Electronic resources}

The companion paper will be linked from my page on partial inconsistency and vector semantics
of programming languages:\\[2ex]

\href{http://www.cs.brandeis.edu/~bukatin/partial\_inconsistency.html}{ http://www.cs.brandeis.edu/$\sim$bukatin/partial\_inconsistency.html}\\[2ex]

It will also be linked from our open source project which implements core DMM primitives in Clojure:\\[2ex]

\href{https://github.com/jsa-aerial/DMM}{https://github.com/jsa-aerial/DMM}

\end{frame}

\end{document}
